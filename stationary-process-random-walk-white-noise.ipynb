{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stationary proess/weak stationary process\n",
    "\n",
    "Strong stationarity concerns the shift-invariance (in time) of its\n",
    "finite-dimensional distributions.\n",
    "\n",
    "Weak stationarity only concerns the shift-invariance (in time) of\n",
    "first and second moments of a process.\n",
    "\n",
    "wiki: \n",
    "In statistics, the term __higher-order statistics__ (HOS) refers to functions which use the third or higher power of a sample, as opposed to more conventional techniques of lower-order statistics, which use constant, linear, and quadratic terms (zeroth, first, and second powers). The third and higher moments, as used in the skewness and kurtosis, are examples of HOS, whereas the first and second moments, as used in the arithmetic mean (first), and variance (second) are examples of low-order statistics. HOS are particularly used in estimation of shape parameters, such as skewness and kurtosis, as when measuring the deviation of a distribution from the normal distribution. On the other hand, due to the higher powers, HOS are significantly less robust than lower-order statistics.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### random walk\n",
    "\n",
    "What is random walk process in time series?\n",
    "One of the simplest and yet most important models in time series forecasting is the random walk model. This model assumes that in each period the variable takes a random step away from its previous value, and the steps are independently and identically distributed in size (“i.i.d.”)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### white noise\n",
    "\n",
    "\n",
    "Consider a time series . If the elements of the series, , are independent and identically distributed (i.i.d.), with a mean of zero, variance  and no serial correlation (i.e. ) then we say that the time series is discrete white noise (DWN).\n",
    "\n",
    "In particular, if the values  are drawn from a standard normal distribution (i.e. ), then the series is known as Gaussian White Noise.\n",
    "\n",
    "White Noise is useful in many contexts. In particular, it can be used to simulate a \"synthetic\" series.\n",
    "\n",
    "As we've mentioned before, a historical time series is only one observed instance. If we can simulate multiple realisations then we can create \"many histories\" and thus generate statistics for some of the parameters of particular models. This will help us refine our models and thus increase accuracy in our forecasting.\n",
    "\n",
    "Now that we've defined Discrete White Noise, we are going to examine some of the attributes of it, including its second order properties and its correlogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### White Noise and Random Walks in Time Series Analysis\n",
    "\n",
    "https://www.quantstart.com/articles/White-Noise-and-Random-Walks-in-Time-Series-Analysis/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### auto-covariance & auto-correlation\n",
    "\n",
    "\n",
    "To understand the difference between __auto-covariance__ and __auto-correlation__, it helps to understand the difference between covariance and correlation.\n",
    "\n",
    "__Covariance__ is a measure of how much two paired variables v1 and v2 vary in the same way/direction. It is positive when v1 is above its mean at the same time that v2 is above its mean and/or v1 is below its mean at the same time that v2 is below its mean. It is negative when the opposite happens, i.e. whenever v1 is above its mean, v2 is below its mean and vice versa.\n",
    "\n",
    "It is important to note that covariance only gives you an idea of the direction of the relationship but its hard to interpret the magnitude of the relation, since it is very dependent on the units that are used. (For instance, if your variables are in cm and then you transform it into inches, the absolute value of the covariance will be very different.\n",
    "\n",
    "__Correlation__ addresses this by scaling the covariance, and putting it into the interval between -1 and 1. Correlation of 1 means your two variables are perfectly positively correlated, -1 means they are perfectly negatively correlated (whenever v1 goes up, v2 goes down), 0 means that there is no correlation at all.\n",
    "\n",
    "Now, for time-series, the \"auto-\" prefix indicates that you calculate the covariance and correlation between one variable at time t1 and the the same variable at a later time t1+k. E.g. for k=1, you calculate the covariance and correlation between one time and the next time. The k indicates the difference or lag between the time points, e.g. if you had monthly data, with k=12 you would inspect the relationship between the variables in the first year and the following year.\n",
    "\n",
    "The auto-correlation coefficients then give you the auto-correlation for each lag k. Comparing the coefficients for different lags can tell you if there is seasonality in the data - e.g. temperatures tend to change with seasons, so you would expect a higher autocorrelation coefficient at around k=12 for monthly data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
